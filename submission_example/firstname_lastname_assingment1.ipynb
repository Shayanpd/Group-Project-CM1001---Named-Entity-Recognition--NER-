{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: This is the title of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:  \n",
    "Shayan Paydar Darian and \n",
    "Daniel Pazirai \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is recommended to start with general import statements\n",
    "from utility_functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should load the raw dataset for the task.  \n",
    "Remember to use relative paths to load any files in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           sid                                           sentence  \\\n",
      "0      1177_0  Memantin ( Ebixa ) ger sällan några biverkningar.   \n",
      "1      1177_1  Det är också lättare att dosera [ flytande med...   \n",
      "2      1177_2  ( Förstoppning ) är ett vanligt problem hos äl...   \n",
      "3      1177_3  [ Medicinen ] kan också göra att man blöder lä...   \n",
      "4      1177_4  Barn har större möjligheter att samarbeta om d...   \n",
      "..        ...                                                ...   \n",
      "922  1177_922  Ofta kan man ha flera besvär i olika delar av ...   \n",
      "923  1177_923  Vaccinationen ger inte ett fullständigt skydd ...   \n",
      "924  1177_924  Vissa personer kan märka av klara förbättringa...   \n",
      "925  1177_925  ( Demens ) innebär dels problem med minnet och...   \n",
      "926  1177_926                                                      \n",
      "\n",
      "                                              entities  \n",
      "0    {'start': [9], 'end': [18], 'text': ['Ebixa'],...  \n",
      "1    {'start': [32], 'end': [52], 'text': ['flytand...  \n",
      "2    {'start': [0], 'end': [16], 'text': ['Förstopp...  \n",
      "3    {'start': [0, 74], 'end': [13, 85], 'text': ['...  \n",
      "4     {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "..                                                 ...  \n",
      "922  {'start': [46], 'end': [57], 'text': ['kroppen...  \n",
      "923   {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "924   {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "925  {'start': [0, 79], 'end': [10, 90], 'text': ['...  \n",
      "926   {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "\n",
      "[927 rows x 3 columns]>\n",
      "<bound method NDFrame.head of               sid                                           sentence  \\\n",
      "0            lt_0  , (hjärtinfarkt) och (syndrom) som vi nu år 19...   \n",
      "1            lt_1  tinernas goda effekt på morbiditeten är välkän...   \n",
      "2            lt_2  [sukralfat], [lakrits] och vismut) som kunde u...   \n",
      "3            lt_3   och tveksamhet {vad} gäller operationsindikat...   \n",
      "4            lt_4   1989 blev en anmälningspliktig (sjukdom) enli...   \n",
      "...           ...                                                ...   \n",
      "745748  lt_745748  iserade rikssjukvården genom att definiera {va...   \n",
      "745749  lt_745749  (glykemi) och (hypertoni). Till detta kan säga...   \n",
      "745750  lt_745750  en för att se {vad} som hänt. Under andra värl...   \n",
      "745751  lt_745751  cent i form av en invasiv (mola) och 10-30 pro...   \n",
      "745752  lt_745752  onskraften från marken minskas eller genom att...   \n",
      "\n",
      "                                                 entities  \n",
      "0       {'start': [2, 21], 'end': [16, 30], 'text': ['...  \n",
      "1        {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "2       {'start': [0, 13], 'end': [11, 22], 'text': ['...  \n",
      "3       {'start': [16], 'end': [21], 'text': ['vad'], ...  \n",
      "4       {'start': [32], 'end': [41], 'text': ['sjukdom...  \n",
      "...                                                   ...  \n",
      "745748  {'start': [43], 'end': [48], 'text': ['vad'], ...  \n",
      "745749  {'start': [0, 14], 'end': [9, 25], 'text': ['g...  \n",
      "745750  {'start': [14], 'end': [19], 'text': ['vad'], ...  \n",
      "745751  {'start': [26], 'end': [32], 'text': ['mola'],...  \n",
      "745752  {'start': [47], 'end': [54], 'text': ['benet']...  \n",
      "\n",
      "[745753 rows x 3 columns]>\n",
      "<bound method NDFrame.head of               sid                                           sentence  \\\n",
      "0          wiki_0  {kropp} beskrivs i till exempel människokroppe...   \n",
      "1          wiki_1  sju miljoner år gammalt hominint {kranium}, kl...   \n",
      "2          wiki_2   autosomer och ett par könskromosomer. Varje {...   \n",
      "3          wiki_3   {kromosom} består av en DNA-molekyl och {prot...   \n",
      "4          wiki_4  tikel:Människans {skelett} Människans skelett ...   \n",
      "...           ...                                                ...   \n",
      "48715  wiki_48715  ingsrelaterade (skador) och sjukdomar. Än idag...   \n",
      "48716  wiki_48716  , (svält), med mera. Andra världskriget är det...   \n",
      "48717  wiki_48717   halva världens [BNP]. Den nyvunna positionen ...   \n",
      "48718  wiki_48718  m {ryggen} på den Polska regeringen och fick b...   \n",
      "48719  wiki_48719  klar (punkt) på efterkrigsagendan, både inom ö...   \n",
      "\n",
      "                                                entities  \n",
      "0      {'start': [0], 'end': [7], 'text': ['kropp'], ...  \n",
      "1      {'start': [33], 'end': [42], 'text': ['kranium...  \n",
      "2      {'start': [45], 'end': [55], 'text': ['kromoso...  \n",
      "3      {'start': [1], 'end': [50], 'text': ['kromosom...  \n",
      "4      {'start': [17], 'end': [26], 'text': ['skelett...  \n",
      "...                                                  ...  \n",
      "48715  {'start': [15], 'end': [23], 'text': ['skador'...  \n",
      "48716  {'start': [2], 'end': [9], 'text': ['svält'], ...  \n",
      "48717  {'start': [16], 'end': [21], 'text': ['BNP'], ...  \n",
      "48718  {'start': [2], 'end': [10], 'text': ['ryggen']...  \n",
      "48719  {'start': [5], 'end': [12], 'text': ['punkt'],...  \n",
      "\n",
      "[48720 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Load the Swedish Medical NER dataset\n",
    "import pandas as pd\n",
    "\n",
    "df_1177 = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/1177/train-00000-of-00001.parquet\")\n",
    "\n",
    "df_lt = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/lt/train-00000-of-00001.parquet\")\n",
    "\n",
    "df_wiki = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/wiki/train-00000-of-00001.parquet\")\n",
    "\n",
    "combined_df = pd.concat([df_1177, df_lt, df_wiki], ignore_index=True)\n",
    "\n",
    "print(df_1177.head)\n",
    "print(df_lt.head)\n",
    "print(df_wiki.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the headers, the data appears to not be tokenized (which we need to do to train the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 927 entries, 0 to 926\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sid       927 non-null    object\n",
      " 1   sentence  927 non-null    object\n",
      " 2   entities  927 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 21.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 745753 entries, 0 to 745752\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   sid       745753 non-null  object\n",
      " 1   sentence  745753 non-null  object\n",
      " 2   entities  745753 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 17.1+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48720 entries, 0 to 48719\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sid       48720 non-null  object\n",
      " 1   sentence  48720 non-null  object\n",
      " 2   entities  48720 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_1177.info())\n",
    "print(df_lt.info())\n",
    "print(df_wiki.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Survey (kolla upp vilka modeller som finns och kolla vilken LLN som är bäst för Swedish Biomedical NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey of Models for Swedish Biomedical NER\n",
    "\n",
    "| **Model**        | **Language(s)** | **Strengths**                                   | **Weaknesses**                                  | **Relevance for Biomedical NER (Swedish)**               |\n",
    "|------------------|-----------------|-------------------------------------------------|-------------------------------------------------|----------------------------------------------------------|\n",
    "| **KB-BERT**      | Swedish         | Native Swedish, decent at general NER tasks     | No biomedical pretraining                       | Good starting point                                      |\n",
    "| **Swedish ALBERT** | Swedish       | Lightweight, faster to train                    | Lacks domain specialization                    | OK, but may miss nuances                                 |\n",
    "| **mBERT**        | Multilingual    | Cross-lingual abilities                         | Not optimized for any specific language/domain | Medium                                                   |\n",
    "| **XLM-RoBERTa**  | Multilingual    | Robust multilingual understanding               | Same as mBERT, no biomedical specialization    | Medium                                                   |\n",
    "| **BioBERT (English)** | English    | Biomedical domain specialized                   | Needs translated data, loses Swedish nuances   | Very good if translation is perfect                      |\n",
    " \n",
    " The survey results are based on searches on HuggingFace and google scholar with keywords (Swedish NER, Biomedical NER, etc), in order to see what models there are and what models have been used before.\n",
    "\n",
    "\n",
    "## Approach and Motivation\n",
    "In our approach, we want to use a Swedish Language-Model rather than translating the existing data into English. Although models like BioBERT offer very good performance in biomedical NER tasks, translation of clinical text can introduce errors and result in a loss of contextual information. This is particularly risky in the biomedical domain, where subtle differences in terminology can significantly impact the meaning of text. Additionally, as we are not biomedical experts, manually validating translations would be challenging.\n",
    "\n",
    "Therefore, our approach is to fine-tune an existing Swedish model for biomedical NER, using the Swedish Medical NER dataset provided. \n",
    "\n",
    "The three models found on HuggingFace which were relevant were: \n",
    "\n",
    "bert-base-swedish-cased (v1) - A BERT trained with the same hyperparameters as first published by Google.\n",
    "\n",
    "bert-base-swedish-cased-ner (experimental) - a BERT fine-tuned for NER using SUC 3.0.\n",
    "\n",
    "albert-base-swedish-cased-alpha (alpha) - A first attempt at an ALBERT for Swedish.\n",
    "\n",
    "## Our Choice\n",
    "Out of of these models, we will be using the base model (bert-base-swedish-cased (v1)). Although the experimental version is fine tuned for NER, we want to train it on specifically biomedical NER. The base model will be perfect for that since it's flexible and gives us more control of fine tuning it and has less prior bias. We will also not use the Albert version since it's a first attempt at it and still in alpha.\n",
    "\n",
    "Link for the KB bert models: https://huggingface.co/KB/bert-base-swedish-cased-ner \n",
    "\n",
    "Study which used KB/bert-base-swedish-cased and\n",
    "bert-base-multilingual-cased: (Although not related to biomedicine) https://aclanthology.org/2024.caldpseudo-1.7.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also load an autotokenizer to tokenize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=\"KB/bert-base-swedish-cased\")\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"KB/bert-base-swedish-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/my-swedish-ner-model-untrained\\\\tokenizer_config.json',\n",
       " './models/my-swedish-ner-model-untrained\\\\special_tokens_map.json',\n",
       " './models/my-swedish-ner-model-untrained\\\\vocab.txt',\n",
       " './models/my-swedish-ner-model-untrained\\\\added_tokens.json',\n",
       " './models/my-swedish-ner-model-untrained\\\\tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model:\n",
    "\n",
    "# Where you want to save it\n",
    "output_dir = \"./models/my-swedish-ner-model-untrained\"\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# Save tokenizer (important!)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Tokenize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define a function to extract unique labels from the dataset\n",
    "def prepare_labels(df):\n",
    "    \"\"\"Extract unique labels from the dataset\"\"\"\n",
    "    all_labels = set()\n",
    "    for entities in df['entities']:\n",
    "        if isinstance(entities, dict) and 'type' in entities:\n",
    "            all_labels.update(entities['type'])\n",
    "    return ['O'] + list(all_labels)  # Add 'O' for outside/no entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Combine the datasets\n",
    "combined_df = pd.concat([df_1177, df_lt, df_wiki], ignore_index=True)\n",
    "\n",
    "# Step 2: Tokenize the sentences\n",
    "# You can use the tokenizer (e.g., from Hugging Face) to tokenize the text\n",
    "label_list = prepare_labels(combined_df)\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "# Check if tokenization is successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out to see if its working:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3/4: (ej klar men vi ska göra modellen och datan redo att tränas)\n",
    "\n",
    "Preprocess the Dataset for Token Classification\n",
    "Most likely, the data will be in token-label format. HuggingFace NER models expect:\n",
    "\n",
    "Input IDs\n",
    "Attention Masks\n",
    "Labels per token\n",
    "I can walk you through the actual code, but at a high level:\n",
    "\n",
    "Tokenize the sentences.\n",
    "Align the labels with the tokens.\n",
    "Format into a HuggingFace dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a function to compute precision, recall, and F1-score\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Compute F1, precision, recall\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] != -100:  # Ignore padding\n",
    "                true_labels.append(labels[i][j])\n",
    "                true_predictions.append(predictions[i][j])\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, true_predictions, average=\"weighted\")\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define a function to tokenize text and align labels\n",
    "def tokenize_and_align_labels(example, tokenizer, label_to_id):\n",
    "    \"\"\"Tokenize text and align labels\"\"\"\n",
    "    sentence = example['sentence']\n",
    "    entities = example['entities']\n",
    "    \n",
    "    # Tokenize sentence\n",
    "    tokenized = tokenizer(sentence, truncation=True, max_length=512)\n",
    "    \n",
    "    # Initialize labels\n",
    "    labels = [label_to_id['O']] * len(tokenized['input_ids'])\n",
    "    \n",
    "    # Handle entities\n",
    "    if isinstance(entities, dict):\n",
    "        starts = entities.get('start', [])\n",
    "        ends = entities.get('end', [])\n",
    "        texts = entities.get('text', [])\n",
    "        types = entities.get('type', [])\n",
    "        \n",
    "        # Align entities with tokens\n",
    "        for start, end, text, entity_type in zip(starts, ends, texts, types):\n",
    "            if start is not None and end is not None:\n",
    "                # Tokenize and find corresponding token indices\n",
    "                word_ids = tokenizer(sentence, truncation=True, max_length=512).word_ids()\n",
    "                entity_token_indices = [\n",
    "                    idx for idx, word_id in enumerate(word_ids) \n",
    "                    if word_id is not None and start <= word_id < end\n",
    "                ]\n",
    "                \n",
    "                # Assign entity label\n",
    "                for idx in entity_token_indices:\n",
    "                    labels[idx] = label_to_id.get(entity_type, label_to_id['O'])\n",
    "    \n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define a function to prepare the dataset for training\n",
    "def prepare_dataset(df, tokenizer, label_to_id):\n",
    "    \"\"\"Prepare dataset for training\"\"\"\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Tokenize and align labels\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda example: tokenize_and_align_labels(example, tokenizer, label_to_id), \n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 636320/636320 [01:40<00:00, 6345.75 examples/s]\n",
      "Map: 100%|██████████| 159080/159080 [00:26<00:00, 6014.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Split data into training and validation sets\n",
    "train_df, val_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare training and validation datasets\n",
    "train_dataset = prepare_dataset(train_df, tokenizer, label_to_id)\n",
    "val_dataset = prepare_dataset(val_df, tokenizer, label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Initialize model with correct number of labels\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"KB/bert-base-swedish-cased\", \n",
    "    num_labels=len(label_list)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Initialize Data Collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Set the number of CPU threads (multi-threading)\n",
    "torch.set_num_threads(24)  # Adjust based on your number of CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./swedish_medical_ner_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,  # Adjusted for training speed\n",
    "    per_device_eval_batch_size=32,  # Adjusted for evaluation speed\n",
    "    num_train_epochs=1,  # Reduce epochs for faster training\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    dataloader_num_workers=0,\n",
    "    use_cpu=True  # Force using CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  # Enables F1-score, precision, recall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19885' max='19885' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19885/19885 5:15:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.015236</td>\n",
       "      <td>0.992901</td>\n",
       "      <td>0.992872</td>\n",
       "      <td>0.992884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19885, training_loss=0.03467170366666591, metrics={'train_runtime': 18955.4742, 'train_samples_per_second': 33.569, 'train_steps_per_second': 1.049, 'total_flos': 9114620288650752.0, 'train_loss': 0.03467170366666591, 'epoch': 1.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 13: Train the model\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/swedish_medical_ner_model_final\\\\tokenizer_config.json',\n",
       " './models/swedish_medical_ner_model_final\\\\special_tokens_map.json',\n",
       " './models/swedish_medical_ner_model_final\\\\vocab.txt',\n",
       " './models/swedish_medical_ner_model_final\\\\added_tokens.json',\n",
       " './models/swedish_medical_ner_model_final\\\\tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 14: Save the model and tokenizer\n",
    "output_dir = \"./models/swedish_medical_ner_model_final\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4972' max='4972' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4972/4972 15:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.015236140228807926, 'eval_precision': 0.9929012667925522, 'eval_recall': 0.9928718190138356, 'eval_f1': 0.9928836343951987, 'eval_runtime': 956.6613, 'eval_samples_per_second': 166.287, 'eval_steps_per_second': 5.197, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 15: Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Mapping:\n",
      "O: 0\n",
      "0: 1\n",
      "1: 2\n",
      "2: 3\n"
     ]
    }
   ],
   "source": [
    "# Step 16: Print label mapping\n",
    "print(\"\\nLabel Mapping:\")\n",
    "for label, idx in label_to_id.items():\n",
    "    print(f\"{label}: {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test down below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Evaluera och kolla var den failade, vilka resultat hade den? etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should contain:\n",
    "- Results.\n",
    "- Summary of best model performance:\n",
    "    - Name of best model file as saved in /models.\n",
    "    - Relevant scores such as: accuracy, precision, recall, F1-score, etc.\n",
    "- Key discussion points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always use comments in the code to document specific steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swedish-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
