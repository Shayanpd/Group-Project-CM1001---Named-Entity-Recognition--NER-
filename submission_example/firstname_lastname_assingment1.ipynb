{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: This is the title of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:  \n",
    "Shayan Paydar Darian and \n",
    "Daniel Pazirai \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n",
      "4.49.0\n",
      "3.4.0\n",
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "# It is recommended to start with general import statements\n",
    "from utility_functions import *\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(pd.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should load the raw dataset for the task.  \n",
    "Remember to use relative paths to load any files in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           sid                                           sentence  \\\n",
      "0      1177_0  Memantin ( Ebixa ) ger sällan några biverkningar.   \n",
      "1      1177_1  Det är också lättare att dosera [ flytande med...   \n",
      "2      1177_2  ( Förstoppning ) är ett vanligt problem hos äl...   \n",
      "3      1177_3  [ Medicinen ] kan också göra att man blöder lä...   \n",
      "4      1177_4  Barn har större möjligheter att samarbeta om d...   \n",
      "..        ...                                                ...   \n",
      "922  1177_922  Ofta kan man ha flera besvär i olika delar av ...   \n",
      "923  1177_923  Vaccinationen ger inte ett fullständigt skydd ...   \n",
      "924  1177_924  Vissa personer kan märka av klara förbättringa...   \n",
      "925  1177_925  ( Demens ) innebär dels problem med minnet och...   \n",
      "926  1177_926                                                      \n",
      "\n",
      "                                              entities  \n",
      "0    {'start': [9], 'end': [18], 'text': ['Ebixa'],...  \n",
      "1    {'start': [32], 'end': [52], 'text': ['flytand...  \n",
      "2    {'start': [0], 'end': [16], 'text': ['Förstopp...  \n",
      "3    {'start': [0, 74], 'end': [13, 85], 'text': ['...  \n",
      "4     {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "..                                                 ...  \n",
      "922  {'start': [46], 'end': [57], 'text': ['kroppen...  \n",
      "923   {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "924   {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "925  {'start': [0, 79], 'end': [10, 90], 'text': ['...  \n",
      "926   {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "\n",
      "[927 rows x 3 columns]>\n",
      "<bound method NDFrame.head of               sid                                           sentence  \\\n",
      "0            lt_0  , (hjärtinfarkt) och (syndrom) som vi nu år 19...   \n",
      "1            lt_1  tinernas goda effekt på morbiditeten är välkän...   \n",
      "2            lt_2  [sukralfat], [lakrits] och vismut) som kunde u...   \n",
      "3            lt_3   och tveksamhet {vad} gäller operationsindikat...   \n",
      "4            lt_4   1989 blev en anmälningspliktig (sjukdom) enli...   \n",
      "...           ...                                                ...   \n",
      "745748  lt_745748  iserade rikssjukvården genom att definiera {va...   \n",
      "745749  lt_745749  (glykemi) och (hypertoni). Till detta kan säga...   \n",
      "745750  lt_745750  en för att se {vad} som hänt. Under andra värl...   \n",
      "745751  lt_745751  cent i form av en invasiv (mola) och 10-30 pro...   \n",
      "745752  lt_745752  onskraften från marken minskas eller genom att...   \n",
      "\n",
      "                                                 entities  \n",
      "0       {'start': [2, 21], 'end': [16, 30], 'text': ['...  \n",
      "1        {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "2       {'start': [0, 13], 'end': [11, 22], 'text': ['...  \n",
      "3       {'start': [16], 'end': [21], 'text': ['vad'], ...  \n",
      "4       {'start': [32], 'end': [41], 'text': ['sjukdom...  \n",
      "...                                                   ...  \n",
      "745748  {'start': [43], 'end': [48], 'text': ['vad'], ...  \n",
      "745749  {'start': [0, 14], 'end': [9, 25], 'text': ['g...  \n",
      "745750  {'start': [14], 'end': [19], 'text': ['vad'], ...  \n",
      "745751  {'start': [26], 'end': [32], 'text': ['mola'],...  \n",
      "745752  {'start': [47], 'end': [54], 'text': ['benet']...  \n",
      "\n",
      "[745753 rows x 3 columns]>\n",
      "<bound method NDFrame.head of               sid                                           sentence  \\\n",
      "0          wiki_0  {kropp} beskrivs i till exempel människokroppe...   \n",
      "1          wiki_1  sju miljoner år gammalt hominint {kranium}, kl...   \n",
      "2          wiki_2   autosomer och ett par könskromosomer. Varje {...   \n",
      "3          wiki_3   {kromosom} består av en DNA-molekyl och {prot...   \n",
      "4          wiki_4  tikel:Människans {skelett} Människans skelett ...   \n",
      "...           ...                                                ...   \n",
      "48715  wiki_48715  ingsrelaterade (skador) och sjukdomar. Än idag...   \n",
      "48716  wiki_48716  , (svält), med mera. Andra världskriget är det...   \n",
      "48717  wiki_48717   halva världens [BNP]. Den nyvunna positionen ...   \n",
      "48718  wiki_48718  m {ryggen} på den Polska regeringen och fick b...   \n",
      "48719  wiki_48719  klar (punkt) på efterkrigsagendan, både inom ö...   \n",
      "\n",
      "                                                entities  \n",
      "0      {'start': [0], 'end': [7], 'text': ['kropp'], ...  \n",
      "1      {'start': [33], 'end': [42], 'text': ['kranium...  \n",
      "2      {'start': [45], 'end': [55], 'text': ['kromoso...  \n",
      "3      {'start': [1], 'end': [50], 'text': ['kromosom...  \n",
      "4      {'start': [17], 'end': [26], 'text': ['skelett...  \n",
      "...                                                  ...  \n",
      "48715  {'start': [15], 'end': [23], 'text': ['skador'...  \n",
      "48716  {'start': [2], 'end': [9], 'text': ['svält'], ...  \n",
      "48717  {'start': [16], 'end': [21], 'text': ['BNP'], ...  \n",
      "48718  {'start': [2], 'end': [10], 'text': ['ryggen']...  \n",
      "48719  {'start': [5], 'end': [12], 'text': ['punkt'],...  \n",
      "\n",
      "[48720 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Load the Swedish Medical NER dataset\n",
    "import pandas as pd\n",
    "\n",
    "df_1177 = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/1177/train-00000-of-00001.parquet\")\n",
    "\n",
    "df_lt = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/lt/train-00000-of-00001.parquet\")\n",
    "\n",
    "df_wiki = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/wiki/train-00000-of-00001.parquet\")\n",
    "\n",
    "print(df_1177.head)\n",
    "print(df_lt.head)\n",
    "print(df_wiki.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('raw_data', exist_ok=True)\n",
    "\n",
    "# Save the DataFrames as parquet files\n",
    "df_1177.to_parquet('raw_data/df_1177.parquet')\n",
    "df_lt.to_parquet('raw_data/df_lt.parquet')\n",
    "df_wiki.to_parquet('raw_data/df_wiki.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 927 entries, 0 to 926\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sid       927 non-null    object\n",
      " 1   sentence  927 non-null    object\n",
      " 2   entities  927 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 21.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 745753 entries, 0 to 745752\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   sid       745753 non-null  object\n",
      " 1   sentence  745753 non-null  object\n",
      " 2   entities  745753 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 17.1+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48720 entries, 0 to 48719\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sid       48720 non-null  object\n",
      " 1   sentence  48720 non-null  object\n",
      " 2   entities  48720 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_1177.info())\n",
    "print(df_lt.info())\n",
    "print(df_wiki.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Survey (kolla upp vilka modeller som finns och kolla vilken LLN som är bäst för Swedish Biomedical NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey of Models for Swedish Biomedical NER\n",
    "\n",
    "| **Model**        | **Language(s)** | **Strengths**                                   | **Weaknesses**                                  | **Relevance for Biomedical NER (Swedish)**               |\n",
    "|------------------|-----------------|-------------------------------------------------|-------------------------------------------------|----------------------------------------------------------|\n",
    "| **KB-BERT**      | Swedish         | Native Swedish, decent at general NER tasks     | No biomedical pretraining                       | Good starting point                                      |\n",
    "| **Swedish ALBERT** | Swedish       | Lightweight, faster to train                    | Lacks domain specialization                    | OK, but may miss nuances                                 |\n",
    "| **mBERT**        | Multilingual    | Cross-lingual abilities                         | Not optimized for any specific language/domain | Medium                                                   |\n",
    "| **XLM-RoBERTa**  | Multilingual    | Robust multilingual understanding               | Same as mBERT, no biomedical specialization    | Medium                                                   |\n",
    "| **BioBERT (English)** | English    | Biomedical domain specialized                   | Needs translated data, loses Swedish nuances   | Very good if translation is perfect                      |\n",
    " \n",
    " The survey results are based on searches on HuggingFace and google scholar with keywords (Swedish NER, Biomedical NER, etc), in order to see what models there are and what models have been used before.\n",
    "\n",
    "\n",
    "## Approach and Motivation\n",
    "In our approach, we want to use a Swedish Language-Model rather than translating the existing data into English. Although models like BioBERT offer very good performance in biomedical NER tasks, translation of clinical text can introduce errors and result in a loss of contextual information. This is particularly risky in the biomedical domain, where subtle differences in terminology can significantly impact the meaning of text. Additionally, as we are not biomedical experts, manually validating translations would be challenging.\n",
    "\n",
    "Therefore, our approach is to fine-tune an existing Swedish model for biomedical NER, using the Swedish Medical NER dataset provided. \n",
    "\n",
    "The three models found on HuggingFace which were relevant were: \n",
    "\n",
    "bert-base-swedish-cased (v1) - A BERT trained with the same hyperparameters as first published by Google.\n",
    "\n",
    "bert-base-swedish-cased-ner (experimental) - a BERT fine-tuned for NER using SUC 3.0.\n",
    "\n",
    "albert-base-swedish-cased-alpha (alpha) - A first attempt at an ALBERT for Swedish.\n",
    "\n",
    "## Our Choice\n",
    "Out of of these models, we will be using the base model (bert-base-swedish-cased (v1)). Although the experimental version is fine tuned for NER, we want to train it on specifically biomedical NER. The base model will be perfect for that since it's flexible and gives us more control of fine tuning it and has less prior bias. We will also not use the Albert version since it's a first attempt at it and still in alpha.\n",
    "\n",
    "Link for the KB bert models: https://huggingface.co/KB/bert-base-swedish-cased-ner \n",
    "\n",
    "Study which used KB/bert-base-swedish-cased and\n",
    "bert-base-multilingual-cased: (Although not related to biomedicine) https://aclanthology.org/2024.caldpseudo-1.7.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is mandatory to maintain the headings for each task.  \n",
    "OPTIONALLY, you can use one level down (###) to organize subsessions of the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use markdown cells like this one to include:\n",
    "- Discussion points.\n",
    "- References to specific sources of code that you might have used to solve the assignment.\n",
    "- General commentas and explanations about your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always use comments in the code to document specific steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Välj modell, ladda ner dataset som står (https://huggingface.co/datasets/community-datasets/swedish_medical_ner) (laddat det med imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should contain the solution of task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Shayan\\.cache\\huggingface\\hub\\models--KB--bert-base-swedish-cased-ner. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=\"KB/bert-base-swedish-cased\")\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"KB/bert-base-swedish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/my-swedish-ner-model-untrained\\\\tokenizer_config.json',\n",
       " './models/my-swedish-ner-model-untrained\\\\special_tokens_map.json',\n",
       " './models/my-swedish-ner-model-untrained\\\\vocab.txt',\n",
       " './models/my-swedish-ner-model-untrained\\\\added_tokens.json',\n",
       " './models/my-swedish-ner-model-untrained\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model:\n",
    "\n",
    "# Where you want to save it\n",
    "output_dir = \"./models/my-swedish-ner-model-untrained\"\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# Save tokenizer (important!)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: \n",
    "\n",
    "Preprocess the Dataset for Token Classification\n",
    "Most likely, the data will be in token-label format. HuggingFace NER models expect:\n",
    "\n",
    "Input IDs\n",
    "Attention Masks\n",
    "Labels per token\n",
    "I can walk you through the actual code, but at a high level:\n",
    "\n",
    "Tokenize the sentences.\n",
    "Align the labels with the tokens.\n",
    "Format into a HuggingFace dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "dataset_1177 = Dataset.from_pandas(df_1177)\n",
    "dataset_lt = Dataset.from_pandas(df_lt)\n",
    "dataset_wiki = Dataset.from_pandas(df_wiki)\n",
    "\n",
    "# Combine all three into one big dataset (optional)\n",
    "dataset = concatenate_datasets([dataset_1177, dataset_lt, dataset_wiki])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:  Load the Model and Tokenizer och Fine-Tunea modellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Evaluera och kolla var den failade, vilka resultat hade den? etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should contain:\n",
    "- Results.\n",
    "- Summary of best model performance:\n",
    "    - Name of best model file as saved in /models.\n",
    "    - Relevant scores such as: accuracy, precision, recall, F1-score, etc.\n",
    "- Key discussion points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always use comments in the code to document specific steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swedish-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
