{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: This is the title of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:  \n",
    "Shayan Paydar Darian and \n",
    "Daniel Pazirai \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n",
      "4.49.0\n",
      "3.4.0\n",
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "# It is recommended to start with general import statements\n",
    "from utility_functions import *\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "print(pd.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should load the raw dataset for the task.  \n",
    "Remember to use relative paths to load any files in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           sid                                           sentence  \\\n",
      "0      1177_0  Memantin ( Ebixa ) ger s√§llan n√•gra biverkningar.   \n",
      "1      1177_1  Det √§r ocks√• l√§ttare att dosera [ flytande med...   \n",
      "2      1177_2  ( F√∂rstoppning ) √§r ett vanligt problem hos √§l...   \n",
      "3      1177_3  [ Medicinen ] kan ocks√• g√∂ra att man bl√∂der l√§...   \n",
      "4      1177_4  Barn har st√∂rre m√∂jligheter att samarbeta om d...   \n",
      "..        ...                                                ...   \n",
      "922  1177_922  Ofta kan man ha flera besv√§r i olika delar av ...   \n",
      "923  1177_923  Vaccinationen ger inte ett fullst√§ndigt skydd ...   \n",
      "924  1177_924  Vissa personer kan m√§rka av klara f√∂rb√§ttringa...   \n",
      "925  1177_925  ( Demens ) inneb√§r dels problem med minnet och...   \n",
      "926  1177_926                                                      \n",
      "\n",
      "                                              entities  \n",
      "0    {'start': [9], 'end': [18], 'text': ['Ebixa'],...  \n",
      "1    {'start': [32], 'end': [52], 'text': ['flytand...  \n",
      "2    {'start': [0], 'end': [16], 'text': ['F√∂rstopp...  \n",
      "3    {'start': [0, 74], 'end': [13, 85], 'text': ['...  \n",
      "4     {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "..                                                 ...  \n",
      "922  {'start': [46], 'end': [57], 'text': ['kroppen...  \n",
      "923   {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "924   {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "925  {'start': [0, 79], 'end': [10, 90], 'text': ['...  \n",
      "926   {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "\n",
      "[927 rows x 3 columns]>\n",
      "<bound method NDFrame.head of               sid                                           sentence  \\\n",
      "0            lt_0  , (hj√§rtinfarkt) och (syndrom) som vi nu √•r 19...   \n",
      "1            lt_1  tinernas goda effekt p√• morbiditeten √§r v√§lk√§n...   \n",
      "2            lt_2  [sukralfat], [lakrits] och vismut) som kunde u...   \n",
      "3            lt_3   och tveksamhet {vad} g√§ller operationsindikat...   \n",
      "4            lt_4   1989 blev en anm√§lningspliktig (sjukdom) enli...   \n",
      "...           ...                                                ...   \n",
      "745748  lt_745748  iserade rikssjukv√•rden genom att definiera {va...   \n",
      "745749  lt_745749  (glykemi) och (hypertoni). Till detta kan s√§ga...   \n",
      "745750  lt_745750  en f√∂r att se {vad} som h√§nt. Under andra v√§rl...   \n",
      "745751  lt_745751  cent i form av en invasiv (mola) och 10-30 pro...   \n",
      "745752  lt_745752  onskraften fr√•n marken minskas eller genom att...   \n",
      "\n",
      "                                                 entities  \n",
      "0       {'start': [2, 21], 'end': [16, 30], 'text': ['...  \n",
      "1        {'start': [], 'end': [], 'text': [], 'type': []}  \n",
      "2       {'start': [0, 13], 'end': [11, 22], 'text': ['...  \n",
      "3       {'start': [16], 'end': [21], 'text': ['vad'], ...  \n",
      "4       {'start': [32], 'end': [41], 'text': ['sjukdom...  \n",
      "...                                                   ...  \n",
      "745748  {'start': [43], 'end': [48], 'text': ['vad'], ...  \n",
      "745749  {'start': [0, 14], 'end': [9, 25], 'text': ['g...  \n",
      "745750  {'start': [14], 'end': [19], 'text': ['vad'], ...  \n",
      "745751  {'start': [26], 'end': [32], 'text': ['mola'],...  \n",
      "745752  {'start': [47], 'end': [54], 'text': ['benet']...  \n",
      "\n",
      "[745753 rows x 3 columns]>\n",
      "<bound method NDFrame.head of               sid                                           sentence  \\\n",
      "0          wiki_0  {kropp} beskrivs i till exempel m√§nniskokroppe...   \n",
      "1          wiki_1  sju miljoner √•r gammalt hominint {kranium}, kl...   \n",
      "2          wiki_2   autosomer och ett par k√∂nskromosomer. Varje {...   \n",
      "3          wiki_3   {kromosom} best√•r av en DNA-molekyl och {prot...   \n",
      "4          wiki_4  tikel:M√§nniskans {skelett} M√§nniskans skelett¬†...   \n",
      "...           ...                                                ...   \n",
      "48715  wiki_48715  ingsrelaterade (skador) och sjukdomar. √Ñn idag...   \n",
      "48716  wiki_48716  , (sv√§lt), med mera. Andra v√§rldskriget √§r det...   \n",
      "48717  wiki_48717   halva v√§rldens [BNP]. Den nyvunna positionen ...   \n",
      "48718  wiki_48718  m {ryggen} p√• den Polska regeringen och fick b...   \n",
      "48719  wiki_48719  klar (punkt) p√• efterkrigsagendan, b√•de inom √∂...   \n",
      "\n",
      "                                                entities  \n",
      "0      {'start': [0], 'end': [7], 'text': ['kropp'], ...  \n",
      "1      {'start': [33], 'end': [42], 'text': ['kranium...  \n",
      "2      {'start': [45], 'end': [55], 'text': ['kromoso...  \n",
      "3      {'start': [1], 'end': [50], 'text': ['kromosom...  \n",
      "4      {'start': [17], 'end': [26], 'text': ['skelett...  \n",
      "...                                                  ...  \n",
      "48715  {'start': [15], 'end': [23], 'text': ['skador'...  \n",
      "48716  {'start': [2], 'end': [9], 'text': ['sv√§lt'], ...  \n",
      "48717  {'start': [16], 'end': [21], 'text': ['BNP'], ...  \n",
      "48718  {'start': [2], 'end': [10], 'text': ['ryggen']...  \n",
      "48719  {'start': [5], 'end': [12], 'text': ['punkt'],...  \n",
      "\n",
      "[48720 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Load the Swedish Medical NER dataset\n",
    "import pandas as pd\n",
    "\n",
    "df_1177 = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/1177/train-00000-of-00001.parquet\")\n",
    "\n",
    "df_lt = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/lt/train-00000-of-00001.parquet\")\n",
    "\n",
    "df_wiki = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/wiki/train-00000-of-00001.parquet\")\n",
    "\n",
    "print(df_1177.head)\n",
    "print(df_lt.head)\n",
    "print(df_wiki.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the headers, the data appears to not be tokenized (which we need to do to train the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('raw_data', exist_ok=True)\n",
    "\n",
    "# Save the DataFrames as parquet files\n",
    "df_1177.to_parquet('raw_data/df_1177.parquet')\n",
    "df_lt.to_parquet('raw_data/df_lt.parquet')\n",
    "df_wiki.to_parquet('raw_data/df_wiki.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 927 entries, 0 to 926\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sid       927 non-null    object\n",
      " 1   sentence  927 non-null    object\n",
      " 2   entities  927 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 21.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 745753 entries, 0 to 745752\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   sid       745753 non-null  object\n",
      " 1   sentence  745753 non-null  object\n",
      " 2   entities  745753 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 17.1+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48720 entries, 0 to 48719\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sid       48720 non-null  object\n",
      " 1   sentence  48720 non-null  object\n",
      " 2   entities  48720 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_1177.info())\n",
    "print(df_lt.info())\n",
    "print(df_wiki.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Survey (kolla upp vilka modeller som finns och kolla vilken LLN som √§r b√§st f√∂r Swedish Biomedical NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey of Models for Swedish Biomedical NER\n",
    "\n",
    "| **Model**        | **Language(s)** | **Strengths**                                   | **Weaknesses**                                  | **Relevance for Biomedical NER (Swedish)**               |\n",
    "|------------------|-----------------|-------------------------------------------------|-------------------------------------------------|----------------------------------------------------------|\n",
    "| **KB-BERT**      | Swedish         | Native Swedish, decent at general NER tasks     | No biomedical pretraining                       | Good starting point                                      |\n",
    "| **Swedish ALBERT** | Swedish       | Lightweight, faster to train                    | Lacks domain specialization                    | OK, but may miss nuances                                 |\n",
    "| **mBERT**        | Multilingual    | Cross-lingual abilities                         | Not optimized for any specific language/domain | Medium                                                   |\n",
    "| **XLM-RoBERTa**  | Multilingual    | Robust multilingual understanding               | Same as mBERT, no biomedical specialization    | Medium                                                   |\n",
    "| **BioBERT (English)** | English    | Biomedical domain specialized                   | Needs translated data, loses Swedish nuances   | Very good if translation is perfect                      |\n",
    " \n",
    " The survey results are based on searches on HuggingFace and google scholar with keywords (Swedish NER, Biomedical NER, etc), in order to see what models there are and what models have been used before.\n",
    "\n",
    "\n",
    "## Approach and Motivation\n",
    "In our approach, we want to use a Swedish Language-Model rather than translating the existing data into English. Although models like BioBERT offer very good performance in biomedical NER tasks, translation of clinical text can introduce errors and result in a loss of contextual information. This is particularly risky in the biomedical domain, where subtle differences in terminology can significantly impact the meaning of text. Additionally, as we are not biomedical experts, manually validating translations would be challenging.\n",
    "\n",
    "Therefore, our approach is to fine-tune an existing Swedish model for biomedical NER, using the Swedish Medical NER dataset provided. \n",
    "\n",
    "The three models found on HuggingFace which were relevant were: \n",
    "\n",
    "bert-base-swedish-cased (v1) - A BERT trained with the same hyperparameters as first published by Google.\n",
    "\n",
    "bert-base-swedish-cased-ner (experimental) - a BERT fine-tuned for NER using SUC 3.0.\n",
    "\n",
    "albert-base-swedish-cased-alpha (alpha) - A first attempt at an ALBERT for Swedish.\n",
    "\n",
    "## Our Choice\n",
    "Out of of these models, we will be using the base model (bert-base-swedish-cased (v1)). Although the experimental version is fine tuned for NER, we want to train it on specifically biomedical NER. The base model will be perfect for that since it's flexible and gives us more control of fine tuning it and has less prior bias. We will also not use the Albert version since it's a first attempt at it and still in alpha.\n",
    "\n",
    "Link for the KB bert models: https://huggingface.co/KB/bert-base-swedish-cased-ner \n",
    "\n",
    "Study which used KB/bert-base-swedish-cased and\n",
    "bert-base-multilingual-cased: (Although not related to biomedicine) https://aclanthology.org/2024.caldpseudo-1.7.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also load an autotokenizer to tokenize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=\"KB/bert-base-swedish-cased\")\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"KB/bert-base-swedish-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/my-swedish-ner-model-untrained\\\\tokenizer_config.json',\n",
       " './models/my-swedish-ner-model-untrained\\\\special_tokens_map.json',\n",
       " './models/my-swedish-ner-model-untrained\\\\vocab.txt',\n",
       " './models/my-swedish-ner-model-untrained\\\\added_tokens.json',\n",
       " './models/my-swedish-ner-model-untrained\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model:\n",
    "\n",
    "# Where you want to save it\n",
    "output_dir = \"./models/my-swedish-ner-model-untrained\"\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# Save tokenizer (important!)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Tokenize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [input_ids, token_type_ids, attention_mask]\n",
      "1    [input_ids, token_type_ids, attention_mask]\n",
      "2    [input_ids, token_type_ids, attention_mask]\n",
      "3    [input_ids, token_type_ids, attention_mask]\n",
      "4    [input_ids, token_type_ids, attention_mask]\n",
      "Name: tokenized, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Combine the datasets\n",
    "combined_df = pd.concat([df_1177, df_lt, df_wiki], ignore_index=True)\n",
    "\n",
    "# Step 2: Tokenize the sentences\n",
    "# You can use the tokenizer (e.g., from Hugging Face) to tokenize the text\n",
    "combined_df['tokenized'] = combined_df['sentence'].apply(lambda x: tokenizer(x, padding=True, truncation=True, max_length=512))\n",
    "\n",
    "# Check if tokenization is successful\n",
    "print(combined_df['tokenized'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out to see if its working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [2, 19224, 15057, 177, 43284, 2456, 49792, 171, 1059, 4407, 704, 27228, 7, 3]\n",
      "Token Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Access the tokenized outputs\n",
    "df = combined_df['tokenized'].iloc[0]\n",
    "\n",
    "# Print individual tokenization components\n",
    "print(\"Input IDs:\", df['input_ids'])\n",
    "print(\"Token Type IDs:\", df['token_type_ids'])\n",
    "print(\"Attention Mask:\", df['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3/4: (ej klar men vi ska g√∂ra modellen och datan redo att tr√§nas)\n",
    "\n",
    "Preprocess the Dataset for Token Classification\n",
    "Most likely, the data will be in token-label format. HuggingFace NER models expect:\n",
    "\n",
    "Input IDs\n",
    "Attention Masks\n",
    "Labels per token\n",
    "I can walk you through the actual code, but at a high level:\n",
    "\n",
    "Tokenize the sentences.\n",
    "Align the labels with the tokens.\n",
    "Format into a HuggingFace dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 636320/636320 [03:51<00:00, 2748.22 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 159080/159080 [01:06<00:00, 2405.12 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1553' max='238620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1553/238620 16:20 < 41:36:42, 1.58 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 130\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Save the model and tokenizer\u001b[39;00m\n\u001b[0;32m    133\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./swedish_medical_ner_model_final\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\transformers\\trainer.py:2599\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2595\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m   2597\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped:\n\u001b[0;32m   2604\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\accelerate\\optimizer.py:178\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\torch\\optim\\adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    210\u001b[0m         group,\n\u001b[0;32m    211\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         state_steps,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\torch\\optim\\adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\anaconda3\\envs\\swedish-ner\\lib\\site-packages\\torch\\optim\\adamw.py:429\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    427\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 429\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "def prepare_labels(df):\n",
    "    \"\"\"Extract unique labels from the dataset\"\"\"\n",
    "    all_labels = set()\n",
    "    for entities in df['entities']:\n",
    "        if isinstance(entities, dict) and 'type' in entities:\n",
    "            all_labels.update(entities['type'])\n",
    "    return ['O'] + list(all_labels)  # Add 'O' for outside/no entity\n",
    "\n",
    "def tokenize_and_align_labels(example, tokenizer, label_to_id):\n",
    "    \"\"\"Tokenize text and align labels\"\"\"\n",
    "    sentence = example['sentence']\n",
    "    entities = example['entities']\n",
    "    \n",
    "    # Tokenize sentence\n",
    "    tokenized = tokenizer(sentence, truncation=True, max_length=512)\n",
    "    \n",
    "    # Initialize labels\n",
    "    labels = [label_to_id['O']] * len(tokenized['input_ids'])\n",
    "    \n",
    "    # Handle entities\n",
    "    if isinstance(entities, dict):\n",
    "        starts = entities.get('start', [])\n",
    "        ends = entities.get('end', [])\n",
    "        texts = entities.get('text', [])\n",
    "        types = entities.get('type', [])\n",
    "        \n",
    "        # Align entities with tokens\n",
    "        for start, end, text, entity_type in zip(starts, ends, texts, types):\n",
    "            if start is not None and end is not None:\n",
    "                # Tokenize and find corresponding token indices\n",
    "                word_ids = tokenizer(sentence, truncation=True, max_length=512).word_ids()\n",
    "                entity_token_indices = [\n",
    "                    idx for idx, word_id in enumerate(word_ids) \n",
    "                    if word_id is not None and start <= word_id < end\n",
    "                ]\n",
    "                \n",
    "                # Assign entity label\n",
    "                for idx in entity_token_indices:\n",
    "                    labels[idx] = label_to_id.get(entity_type, label_to_id['O'])\n",
    "    \n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "def prepare_dataset(df, tokenizer, label_to_id):\n",
    "    \"\"\"Prepare dataset for training\"\"\"\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Tokenize and align labels\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda example: tokenize_and_align_labels(example, tokenizer, label_to_id), \n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "def main():\n",
    "    # Load datasets\n",
    "    df_1177 = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/1177/train-00000-of-00001.parquet\")\n",
    "    df_lt = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/lt/train-00000-of-00001.parquet\")\n",
    "    df_wiki = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/wiki/train-00000-of-00001.parquet\")\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([df_1177, df_lt, df_wiki], ignore_index=True)\n",
    "\n",
    "    # Load pre-trained tokenizer\n",
    "    model_name = \"KB/bert-base-swedish-cased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Prepare labels\n",
    "    label_list = prepare_labels(combined_df)\n",
    "    label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "    id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "    # Split data\n",
    "    train_df, val_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Prepare training and validation datasets\n",
    "    train_dataset = prepare_dataset(train_df, tokenizer, label_to_id)\n",
    "    val_dataset = prepare_dataset(val_df, tokenizer, label_to_id)\n",
    "\n",
    "    # Initialize model with correct number of labels\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=len(label_list)\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./swedish_medical_ner_model\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        use_cpu=False  # Set to False to use GPU if available\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting model training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    output_dir = \"./swedish_medical_ner_model_final\"\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "    # Print label mapping\n",
    "    print(\"\\nLabel Mapping:\")\n",
    "    for label, idx in label_to_id.items():\n",
    "        print(f\"{label}: {idx}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Evaluera och kolla var den failade, vilka resultat hade den? etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should contain:\n",
    "- Results.\n",
    "- Summary of best model performance:\n",
    "    - Name of best model file as saved in /models.\n",
    "    - Relevant scores such as: accuracy, precision, recall, F1-score, etc.\n",
    "- Key discussion points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always use comments in the code to document specific steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swedish-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
